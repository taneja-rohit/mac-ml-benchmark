# Constraints, Learnings & The Bitter Truth About Apple Silicon ML

> "The ships hung in the sky in much the same way that bricks don't."
> â€” Douglas Adams
>
> Apple Silicon handles ML workloads in much the same way.

---

## Table of Contents
1. [The 12GB Wall: MPS Tensor Limits](#the-12gb-wall)
2. [Why Mistral-7B Won't Fit (Math Time)](#why-mistral-7b-wont-fit)
3. [PyTorch MPS vs MLX: WTF is the Difference?](#pytorch-mps-vs-mlx)
4. [Quantization: The Industry's Escape Hatch](#quantization)
5. [Why NVIDIA is Winning](#why-nvidia-is-winning)
6. [Quality vs. Compression Tradeoffs](#quality-tradeoffs)
7. [Apple's Missed Opportunities](#apples-missed-opportunities)
8. [Practical Recommendations](#practical-recommendations)

---

## The 12GB Wall: MPS Tensor Limits {#the-12gb-wall}

### What's Actually Happening

Apple's Metal Performance Shaders (MPS) â€” the backend that makes PyTorch run on Apple GPUs â€” has a dirty little secret: **it uses 32-bit signed integers for tensor dimension indexing**.

```
Maximum elements per dimension = 2^31 - 1 = 2,147,483,647

For a contiguous float16 tensor:
  Max size â‰ˆ 2^31 Ã— 2 bytes = 4 GB per dimension
  
For float32:
  Max size â‰ˆ 2^31 Ã— 4 bytes = 8 GB per dimension

In practice, with overhead: ~12 GB max allocation observed
```

### The Infuriating Part

Your MacBook has **24 GB of unified memory**. The GPU and CPU share it. In theory, you could load a 20GB model. In practice, MPS says "nah."

This is a **software limitation**, not hardware. Apple's Metal team optimized for:
- Video editing (many small textures)
- Image processing (bounded dimensions)
- NOT giant weight matrices sitting in VRAM

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  YOUR MACBOOK'S EXISTENTIAL CRISIS                                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                   â•‘
â•‘   Physical RAM:        24 GB    "I have so much potential!"       â•‘
â•‘   Usable for ML:       12 GB    "But MPS won't let me use it"    â•‘
â•‘   Mistral-7B needs:    14 GB    "So close, yet so far"           â•‘
â•‘                                                                   â•‘
â•‘   Status: PAIN                                                    â•‘
â•‘                                                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Technical Root Cause

```c
// Somewhere in Apple's Metal framework (conceptually):
struct MPSNDArrayDescriptor {
    int32_t dimensions[8];    // <-- HERE'S THE PROBLEM
    // ...
};

// When you try to allocate > INT_MAX elements:
// "NDArray dimension length > INT_MAX" ğŸ’€
```

NVIDIA's CUDA uses `size_t` (64-bit) for this. Apple chose `int32_t`. Presumably in 2016 when 8GB GPUs were exotic.

---

## Why Mistral-7B Won't Fit {#why-mistral-7b-wont-fit}

Let's do the math. Mistral-7B has 7.24 billion parameters.

### Memory Breakdown (float16 inference)

```
MISTRAL-7B ARCHITECTURE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Layers:           32 transformer blocks
Hidden dim:       4096
Intermediate:     14336 (FFN)
Vocab size:       32000
Attention heads:  32
KV heads:         8 (Grouped Query Attention)

WEIGHT SIZES (float16 = 2 bytes per param):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Embedding:        32000 Ã— 4096 Ã— 2 =    256 MB
Per-layer:
  â”œâ”€ Q proj:      4096 Ã— 4096 Ã— 2 =      32 MB
  â”œâ”€ K proj:      4096 Ã— 1024 Ã— 2 =       8 MB  (GQA)
  â”œâ”€ V proj:      4096 Ã— 1024 Ã— 2 =       8 MB  (GQA)
  â”œâ”€ O proj:      4096 Ã— 4096 Ã— 2 =      32 MB
  â”œâ”€ Gate proj:   4096 Ã— 14336 Ã— 2 =    112 MB
  â”œâ”€ Up proj:     4096 Ã— 14336 Ã— 2 =    112 MB
  â””â”€ Down proj:   14336 Ã— 4096 Ã— 2 =    112 MB
  Layer total:                          416 MB Ã— 32 = 13.3 GB

LM Head:          4096 Ã— 32000 Ã— 2 =    256 MB

TOTAL WEIGHTS:    ~14 GB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### Runtime Memory

```
RUNTIME MEMORY (inference, seq_len=2048):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Weights:                               14.0 GB
KV Cache (32 layers Ã— 2048 tokens):     2.0 GB
Activations (batch=1):                  0.5 GB
Framework overhead:                     0.5 GB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:                                ~17.0 GB

MPS LIMIT:                             12.0 GB

VERDICT:                               âŒ DOESN'T FIT
```

---

## PyTorch MPS vs MLX: WTF is the Difference? {#pytorch-mps-vs-mlx}

This is the question everyone has. Let me break it down:

### The Stack Diagram

```
YOUR PYTHON CODE
      â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                                                     â”‚
      â–¼                                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      PyTorch            â”‚                    â”‚         MLX             â”‚
â”‚   (Meta/Facebook)       â”‚                    â”‚       (Apple)           â”‚
â”‚                         â”‚                    â”‚                         â”‚
â”‚  - 10+ years mature     â”‚                    â”‚  - Released Dec 2023    â”‚
â”‚  - Massive ecosystem    â”‚                    â”‚  - Apple-native         â”‚
â”‚  - CUDA-first design    â”‚                    â”‚  - NumPy-like API       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                              â”‚
            â–¼                                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     MPS Backend         â”‚                    â”‚    MLX Metal Backend    â”‚
â”‚  (Apple contribution    â”‚                    â”‚    (Apple internal)     â”‚
â”‚   to PyTorch)           â”‚                    â”‚                         â”‚
â”‚                         â”‚                    â”‚  - Lazy evaluation      â”‚
â”‚  - Bolted-on adapter    â”‚                    â”‚  - Unified memory aware â”‚
â”‚  - Translates CUDA ops  â”‚                    â”‚  - Apple-optimized      â”‚
â”‚    to Metal shaders     â”‚                    â”‚                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                              â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Apple Metal API      â”‚
                    â”‚    (GPU driver)         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Apple Silicon GPU     â”‚
                    â”‚   (M5 - 12 cores)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Differences

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    PyTorch + MPS vs MLX                                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Aspect              â”‚ PyTorch + MPS          â”‚ MLX                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Who maintains it    â”‚ Meta + Apple contribs  â”‚ Apple ML Research          â•‘
â•‘ Maturity            â”‚ 10+ years (MPS: 2022)  â”‚ ~1 year (Dec 2023)         â•‘
â•‘ Design philosophy   â”‚ CUDA-first, MPS bolted â”‚ Apple-native from scratch  â•‘
â•‘ Execution model     â”‚ Eager (immediate)      â”‚ Lazy (deferred)            â•‘
â•‘ Memory model        â”‚ CUDA-style (explicit)  â”‚ Unified memory aware       â•‘
â•‘ API                 â”‚ PyTorch (industry std) â”‚ NumPy-like (simpler)       â•‘
â•‘ Ecosystem           â”‚ Massive (HuggingFace)  â”‚ Growing (mlx-lm, etc)      â•‘
â•‘ float16 performance â”‚ GREAT (13 TFLOPS)      â”‚ Meh (3.5 TFLOPS)           â•‘
â•‘ Quantization        â”‚ bitsandbytes, GPTQ     â”‚ Native 4-bit               â•‘
â•‘ Training support    â”‚ Full                   â”‚ Full (LoRA friendly)       â•‘
â•‘ Documentation       â”‚ Excellent              â”‚ Decent                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Why PyTorch float16 is 3.7x Faster Than MLX

This is the shocking finding from our benchmarks:

```
GEMM 4096x4096:
  PyTorch MPS float16:  13.4 TFLOPS  ğŸ†
  MLX float16:           3.6 TFLOPS  
  
WHY?

1. PyTorch MPS uses MPSGraph
   - Apple's high-level ML graph API
   - Has hand-tuned GEMM kernels for float16
   - Benefits from years of Metal optimization

2. MLX uses raw Metal compute shaders
   - More flexible (lazy eval)
   - But less optimized for peak throughput
   - Still catching up on kernel optimization

3. The MPS float16 path hits Apple's "fast path"
   - Likely uses AMX (Apple Matrix coprocessor)
   - MLX may not be triggering this yet
```

### Where is Major Effort Going?

```
DEVELOPMENT INVESTMENT:

PyTorch (Meta):
â”œâ”€ Main focus: CUDA, ROCm (AMD), XLA (TPU)
â”œâ”€ MPS: ~5% of effort, mostly Apple contributions
â””â”€ Future: torch.compile, inductor backend
   
MLX (Apple):
â”œâ”€ Main focus: Apple Silicon optimization
â”œâ”€ Growing fast: mlx-lm, mlx-vlm, mlx-audio
â””â”€ Future: Unknown (Apple doesn't share roadmaps)

INDUSTRY MOMENTUM:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Framework        â”‚ GitHub Stars â”‚ Contributors â”‚ HF Models
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PyTorch          â”‚ 85k          â”‚ 3,500+       â”‚ 500k+
MLX              â”‚ 18k          â”‚ 100+         â”‚ 1,000+
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

VERDICT: 
- PyTorch has massive momentum, MPS is an afterthought
- MLX is Apple's bet, growing but niche
- For production: PyTorch (ecosystem)
- For Mac-specific: MLX (if you can port)
```

### The Uncomfortable Truth

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                         THE REAL SITUATION                                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                           â•‘
â•‘   Meta (PyTorch):  "MPS? Sure, Apple can maintain that."                 â•‘
â•‘   Apple (MPS):     "We'll do the minimum to not embarrass ourselves."    â•‘
â•‘   Apple (MLX):     "Here's our REAL answer for Apple Silicon."           â•‘
â•‘   ML Community:    "We have 10 million lines of PyTorch code."           â•‘
â•‘   Apple (MLX):     "Cool, rewrite it."                                   â•‘
â•‘   ML Community:    "..."                                                  â•‘
â•‘                                                                           â•‘
â•‘   Result: Everyone uses PyTorch+MPS and complains about it.              â•‘
â•‘                                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Quantization: The Industry's Escape Hatch {#quantization}

### What is Quantization?

Converting weights from high-precision (float16/32) to low-precision (int8/int4):

```
FLOAT16 (16 bits per weight):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ S â”‚ EEEEE â”‚ MMMMMMMMMM â”‚   Range: Â±65504      â”‚
â”‚ 1 â”‚   5   â”‚     10     â”‚   Precision: ~0.001  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INT8 (8 bits per weight):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SSSSSSSS â”‚  Range: -128 to 127              â”‚
â”‚    8     â”‚  + scale factor per group        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INT4 (4 bits per weight):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SSSS â”‚  Range: -8 to 7                      â”‚
â”‚  4   â”‚  + scale + zero-point per group      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Memory Savings

```
MISTRAL-7B MEMORY BY PRECISION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Precision    Size        Fits MPS?    Quality
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
float32      28 GB       âŒ No        100%
float16      14 GB       âŒ No        ~100%
int8          7 GB       âœ… Yes       ~99%
int4 (GPTQ)   4 GB       âœ… Yes       ~97%
int4 (AWQ)    4 GB       âœ… Yes       ~98%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### Industry-Standard Methods

| Method | How it Works | Pros | Cons |
|--------|-------------|------|------|
| **GPTQ** | Calibration-based 4-bit, Hessian-weighted | Fast inference, wide support | Needs calibration data |
| **AWQ** | Activation-aware, protects salient weights | Better quality than GPTQ | Slightly more complex |
| **bitsandbytes** | On-the-fly int8/int4 | Easy integration | Slower than pre-quantized |
| **GGUF** | CPU+GPU hybrid, mixed precision | Runs anywhere | Not optimal for pure GPU |
| **MLX Native** | Apple-optimized 4-bit | Best on Apple Silicon | Apple-only |

---

## Why NVIDIA is Winning {#why-nvidia-is-winning}

### The Technical Gap

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    NVIDIA vs APPLE SILICON                           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Metric              â”‚ NVIDIA H100      â”‚ Apple M5                    â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ FP16 TFLOPS         â”‚ 1,979            â”‚ ~14 (measured)              â•‘
â•‘ Memory              â”‚ 80 GB HBM3       â”‚ 24 GB unified               â•‘
â•‘ Memory BW           â”‚ 3.35 TB/s        â”‚ ~120 GB/s (measured)        â•‘
â•‘ Tensor Cores        â”‚ Yes (4th gen)    â”‚ No                          â•‘
â•‘ Max tensor size     â”‚ 80 GB            â”‚ 12 GB (MPS limit)           â•‘
â•‘ Flash Attention     â”‚ Native           â”‚ Hacky/limited               â•‘
â•‘ CUDA ecosystem      â”‚ Massive          â”‚ N/A                         â•‘
â•‘ Price               â”‚ $30,000          â”‚ $2,499                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### The Ecosystem Moat

```
NVIDIA's unfair advantages:

1. CUDA (2007) - 17 years of momentum
   â””â”€ Every ML framework: "CUDA first, maybe others later"

2. cuDNN - Hand-tuned kernels for every operation
   â””â”€ Apple's MPS: "Here's some generic Metal shaders, good luck"

3. Tensor Cores - Hardware matrix multiply units
   â””â”€ M5 GPU: General-purpose ALUs doing matrix math

4. NVLink/NVSwitch - Multi-GPU at 900 GB/s
   â””â”€ Apple: "Thunderbolt 4 at 40 Gbps, take it or leave it"

5. Software stack depth:
   NVIDIA: CUDA â†’ cuDNN â†’ cuBLAS â†’ TensorRT â†’ Triton â†’ vLLM
   Apple:  Metal â†’ MPS â†’ ... â†’ ... â†’ "it works on MacBooks I guess"
```

---

## Quality vs. Compression Tradeoffs {#quality-tradeoffs}

### Benchmark Reality

```
MISTRAL-7B QUALITY BY QUANTIZATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Benchmark        â”‚ FP16    â”‚ INT8    â”‚ GPTQ-4  â”‚ AWQ-4  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€
MMLU (knowledge) â”‚ 60.1%   â”‚ 59.8%   â”‚ 59.2%   â”‚ 59.5%  
HellaSwag        â”‚ 81.3%   â”‚ 81.2%   â”‚ 80.8%   â”‚ 81.0%  
HumanEval (code) â”‚ 32.0%   â”‚ 31.5%   â”‚ 30.5%   â”‚ 31.2%  
GSM8K (math)     â”‚ 52.2%   â”‚ 51.5%   â”‚ 49.8%   â”‚ 50.9%  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€

Key insight: Knowledge/reasoning barely affected (-0.5%)
             Math/code takes the hit (-3-5%)
```

---

## Apple's Missed Opportunities {#apples-missed-opportunities}

### What Apple Got Right âœ…

- **Unified Memory Architecture** - Could enable massive models on laptops
- **Power Efficiency** - 30W vs 700W for comparable tasks
- **MLX Framework** - Lazy evaluation, NumPy-like API
- **Hardware potential** - Neural Engine exists (16 TOPS)

### What Apple Got Wrong âŒ

- **MPS INT_MAX Limitation** - Inexcusable in 2024
- **No Tensor Cores** - GPU does generic ALU math
- **Neural Engine Locked Down** - 16 TOPS sitting unused, only via CoreML
- **Half-baked Flash Attention** - MPS implementation incomplete
- **Ecosystem Neglect** - PyTorch MPS is community-maintained mostly

---

## Practical Recommendations {#practical-recommendations}

### For Your M5 MacBook (24GB)

```
MODEL SELECTION GUIDE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Model Size (params)  â”‚ Precision â”‚ Memory   â”‚ Verdict
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
< 3B (Phi-2, etc)    â”‚ FP16      â”‚ ~6 GB    â”‚ âœ… Runs great
7B (Mistral, Llama)  â”‚ INT4      â”‚ ~4 GB    â”‚ âœ… Use quantized
7B                   â”‚ FP16      â”‚ ~14 GB   â”‚ âŒ Won't fit
13B                  â”‚ INT4      â”‚ ~7 GB    â”‚ âœ… Works
70B                  â”‚ Any       â”‚ 35+ GB   â”‚ âŒ Forget it
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### Commands to Run

```bash
# MLX 4-bit inference (easiest, best for Mac)
pip install mlx-lm
mlx_lm.generate --model mlx-community/Mistral-7B-Instruct-v0.2-4bit \
                --prompt "Your prompt here"

# PyTorch INT8 via bitsandbytes
pip install bitsandbytes accelerate
# Load with: load_in_8bit=True

# PyTorch GPTQ-4
pip install auto-gptq optimum
# Load TheBloke's GPTQ models from HuggingFace
```

---

## Benchmark Results from This Machine

```
APPLE M5 (24GB) - MEASURED PERFORMANCE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
COMPUTE:
  PyTorch+MPS GEMM (float16):    13.4 TFLOPS (peak)
  PyTorch+MPS GEMM (float32):     3.6 TFLOPS
  MLX GEMM (all precisions):      3.5 TFLOPS
  Attention (seq=2048):           2.9 TFLOPS

MEMORY:
  Bandwidth (copy):             119 GB/s
  Max single allocation:         12 GB

FRAMEWORK WINNER:
  PyTorch+MPS float16 beats MLX by 3.7x on GEMM
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

*"The answer to the ultimate question of ML, the universe, and everything is: use 4-bit quantization and stop complaining about MPS."*

â€” Generated 2026-01-22, Apple M5 (24GB)
