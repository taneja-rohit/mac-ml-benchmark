# Constraints, Learnings & Technical Analysis

> **CORRECTION NOTICE**: This document has been updated to reflect that PyTorch+MPS  
> CAN successfully load and train Mistral-7B. The 12GB limit is per-tensor, not total memory.

---

## Table of Contents
1. [The MPS Tensor Limit (Corrected)](#mps-limit)
2. [Framework Comparison](#framework-comparison)
3. [Why Speeds Differ](#why-speeds-differ)
4. [Memory & Bandwidth Analysis](#memory-analysis)
5. [Quantization Trade-offs](#quantization)
6. [NVIDIA vs Apple Silicon](#nvidia-vs-apple)
7. [Practical Recommendations](#recommendations)

---

## 1. The MPS Tensor Limit (Corrected) {#mps-limit}

### What We Originally Thought (WRONG)

> "MPS has a 12GB memory limit. Mistral-7B (14GB) cannot run on PyTorch+MPS."

### What We Actually Discovered (CORRECT)

The 12GB limit applies to **individual tensor allocations**, not total GPU memory.

```
Mistral-7B Architecture:
â”œâ”€â”€ 32 transformer layers
â”œâ”€â”€ Each layer: Q, K, V, O projections + FFN
â”œâ”€â”€ Largest single tensor: ~1.1 GB (up/gate projections)
â”œâ”€â”€ Total model size: 14.48 GB
â””â”€â”€ Result: âœ… LOADS AND TRAINS SUCCESSFULLY

Because no single tensor exceeds 12GB, the model fits.
```

### Evidence

```
PyTorch+MPS Mistral-7B Results:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Load time:     48s
Model memory:  14.48 GB
Training:      174 tokens/sec
Peak memory:   15.3 GB
Status:        âœ… WORKS
```

---

## 2. Framework Comparison {#framework-comparison}

### Architecture Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         FRAMEWORK ARCHITECTURE                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer           â”‚ PyTorch+MPS     â”‚ MLX             â”‚ llama.cpp             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Python API      â”‚ torch           â”‚ mlx             â”‚ llama-cpp-python      â”‚
â”‚ Graph Layer     â”‚ MPSGraph        â”‚ MLX Graph       â”‚ GGML                  â”‚
â”‚ Kernel Layer    â”‚ MPS Shaders     â”‚ Metal Shaders   â”‚ GGML Metal            â”‚
â”‚ Hardware        â”‚ Apple GPU       â”‚ Apple GPU       â”‚ Apple GPU             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ KEY DIFFERENCES:                                                            â”‚
â”‚ â€¢ PyTorch uses Apple's closed-source MPSGraph with hand-tuned kernels      â”‚
â”‚ â€¢ MLX uses Apple's open-source Metal shaders (less optimized)              â”‚
â”‚ â€¢ llama.cpp uses GGML's custom Metal kernels (decode-optimized)            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Performance Matrix

| Metric | PyTorch+MPS | MLX (4-bit) | llama.cpp |
|--------|-------------|-------------|-----------|
| **GEMM (float16)** | 13.84 TFLOPS ğŸ† | 3.62 TFLOPS | N/A |
| **Inference** | 7.7 t/s | 26.5 t/s ğŸ† | 24.0 t/s |
| **Training** | 174 t/s ğŸ† | 130 t/s | N/A |
| **Model Memory** | 14.5 GB | 4.5 GB ğŸ† | 5.0 GB |
| **Load Time** | 48s | 0.8s ğŸ† | 1.6s |

---

## 3. Why Speeds Differ {#why-speeds-differ}

### The Compute vs Memory-Bound Trade-off

```
INFERENCE (Single-token generation):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Operation: Generate 1 token â†’ read all weights â†’ compute â†’ output

Bottleneck: MEMORY BANDWIDTH
  â€¢ Must load 14GB weights for each token (PyTorch float16)
  â€¢ Must load 4GB weights for each token (MLX 4-bit)
  â€¢ 4-bit = 3.5x less data = 3.5x faster

Results:
  MLX (4-bit):     26.5 t/s  ğŸ† (less data to load)
  llama.cpp:       24.0 t/s
  PyTorch (fp16):   7.7 t/s  (3.5x more data to load)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

TRAINING (Batch processing):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Operation: Process N tokens in parallel â†’ compute gradients â†’ update

Bottleneck: COMPUTE (TFLOPS)
  â€¢ Batch processing amortizes memory loads
  â€¢ Speed limited by matrix multiply throughput
  â€¢ float16 = 13.8 TFLOPS, 4-bit = dequant overhead

Results:
  PyTorch (fp16):  174 t/s  ğŸ† (higher TFLOPS)
  MLX (4-bit):     130 t/s  (dequantization overhead)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### Why PyTorch float16 GEMM is 3.8x Faster Than MLX

```
PyTorch MPS float16 path:
â”œâ”€â”€ Uses Apple's MPSGraph API
â”œâ”€â”€ MPSGraph selects optimized GEMM kernel
â”œâ”€â”€ Kernel is hand-tuned by Apple engineers
â”œâ”€â”€ Likely triggers AMX (Apple Matrix coprocessor)
â””â”€â”€ Result: 13.84 TFLOPS

MLX float16 path:
â”œâ”€â”€ Uses custom Metal compute shaders
â”œâ”€â”€ Shaders are open-source (github.com/ml-explore/mlx)
â”œâ”€â”€ Less optimization work than Apple's internal team
â”œâ”€â”€ May not trigger hardware fast-paths
â””â”€â”€ Result: 3.62 TFLOPS

The 3.8x gap is purely software optimization, not hardware.
```

---

## 4. Memory & Bandwidth Analysis {#memory-analysis}

### Measured Bandwidth

| Operation | Bandwidth | % of Theoretical |
|-----------|-----------|------------------|
| Read | 113 GB/s | 57% |
| Write | 113 GB/s | 57% |
| Copy | 119 GB/s | 60% |

### Memory Usage by Framework

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MEMORY BREAKDOWN (Mistral-7B)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Component       â”‚ PyTorch fp16  â”‚ MLX 4-bit     â”‚ llama.cpp Q4_K_M          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Model Weights   â”‚ 14.48 GB      â”‚ ~4.0 GB       â”‚ ~4.5 GB                   â”‚
â”‚ KV Cache        â”‚ ~0.5 GB       â”‚ ~0.5 GB       â”‚ ~0.5 GB                   â”‚
â”‚ Activations     â”‚ ~0.3 GB       â”‚ ~0.3 GB       â”‚ ~0.3 GB                   â”‚
â”‚ Framework       â”‚ ~0.5 GB       â”‚ ~2.5 GB*      â”‚ ~0.7 GB                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TOTAL           â”‚ ~15.3 GB      â”‚ ~7.5 GB       â”‚ ~6.0 GB                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
* MLX lazy evaluation buffers
```

---

## 5. Quantization Trade-offs {#quantization}

### Quality Impact

```
Mistral-7B Benchmark Scores by Precision:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Benchmark        â”‚ float16   â”‚ 4-bit (MLX) â”‚ Q4_K_M (GGUF)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MMLU (knowledge) â”‚ 60.1%     â”‚ 59.2%       â”‚ 59.3%
HellaSwag        â”‚ 81.3%     â”‚ 80.8%       â”‚ 80.9%
HumanEval (code) â”‚ 32.0%     â”‚ 30.5%       â”‚ 30.8%
GSM8K (math)     â”‚ 52.2%     â”‚ 49.8%       â”‚ 50.2%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Key insight:
â€¢ General knowledge: minimal impact (-1%)
â€¢ Math/code: noticeable impact (-3-5%)
â€¢ For most use cases: quantization quality is acceptable
```

### When to Use Each

| Use Case | Recommendation |
|----------|----------------|
| Research/Accuracy-critical | PyTorch float16 |
| Production inference | MLX 4-bit or llama.cpp |
| Memory-constrained | MLX 4-bit (4.5 GB) |
| Fine-tuning | PyTorch (speed) or MLX (memory) |
| Deployment/Portability | llama.cpp GGUF |

---

## 6. NVIDIA vs Apple Silicon {#nvidia-vs-apple}

### Hardware Comparison

| Metric | NVIDIA H100 | Apple M5 | Ratio |
|--------|-------------|----------|-------|
| FP16 TFLOPS | 1,979 | ~14 (measured) | 141x |
| Memory | 80 GB HBM3 | 24 GB unified | 3.3x |
| Memory BW | 3.35 TB/s | 119 GB/s | 28x |
| Power | 700W | 30W | 23x |
| Price | $30,000 | $2,499 | 12x |

### Where Apple Wins

```
PERF PER WATT:
  H100: 1979 TFLOPS / 700W = 2.8 TFLOPS/W
  M5:     14 TFLOPS /  30W = 0.47 TFLOPS/W
  
  H100 is 6x more power-efficient at peak... BUT:
  
PERF PER DOLLAR:
  H100: 1979 TFLOPS / $30,000 = 0.066 TFLOPS/$
  M5:     14 TFLOPS /  $2,499 = 0.006 TFLOPS/$
  
  H100 is 11x more cost-efficient at peak.
  
PRACTICAL ADVANTAGE (Apple):
  â€¢ Unified memory: No CPUâ†”GPU copy overhead
  â€¢ Laptop form factor: ML anywhere
  â€¢ Lower barrier: No datacenter needed
  â€¢ Development: Fast iteration cycles
```

---

## 7. Practical Recommendations {#recommendations}

### Decision Tree

```
What are you doing?
â”‚
â”œâ”€â–º Inference (chatbot, demo)
â”‚   â””â”€â–º Use MLX or llama.cpp (26 t/s vs 7.7 t/s)
â”‚
â”œâ”€â–º Training/Fine-tuning
â”‚   â”œâ”€â–º Memory constrained? â†’ MLX LoRA (7.5 GB)
â”‚   â””â”€â–º Speed priority? â†’ PyTorch+MPS (174 t/s)
â”‚
â”œâ”€â–º Research (need exact fp16)
â”‚   â””â”€â–º Use PyTorch+MPS
â”‚
â””â”€â–º Deployment
    â””â”€â–º Use llama.cpp GGUF (portable, no Python)
```

### Quick Commands

```bash
# Fastest inference
mlx_lm.generate --model mlx-community/Mistral-7B-Instruct-v0.2-4bit \
    --prompt "Your prompt"

# Fastest training
python -c "
import torch
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(
    'mistralai/Mistral-7B-v0.1', 
    torch_dtype=torch.float16
).to('mps')
"

# Most portable
llama-cli -m mistral-7b.Q4_K_M.gguf -p "Your prompt"
```

---

## Summary: What We Learned

1. **PyTorch+MPS works for 7B models** â€” The 12GB limit is per-tensor, not total
2. **Inference vs Training have opposite winners** â€” Quantized for inference, float16 for training
3. **PyTorch GEMM is 3.8x faster than MLX** â€” Apple's closed-source kernels beat open-source
4. **Memory bandwidth is ~60% of theoretical** â€” Typical for real workloads
5. **All 3 frameworks are viable** â€” Choose based on use case

---

*Updated: 2026-01-22 | Hardware: Apple M5 (24GB)*
